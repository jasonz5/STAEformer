NYCTaxi
Trainset:	x-(1912, 35, 200, 4)	y-(1912, 1, 200, 2)
Valset:  	x-(274, 35, 200, 4)  	y-(274, 1, 200, 2)
Testset:	x-(546, 35, 200, 4)	y-(546, 1, 200, 2)

--------- STAEformer ---------
{
    "num_nodes": 200,
    "in_steps": 35,
    "out_steps": 1,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0003,
    "milestones": [
        20,
        30
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 32,
    "max_epochs": 200,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "dataset": "NYCTaxi",
        "num_nodes": 200,
        "in_steps": 35,
        "out_steps": 1,
        "steps_per_day": 48,
        "input_dim": 4,
        "output_dim": 2,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [32, 1, 200, 2]           560,000
├─Linear: 1-1                            [32, 35, 200, 24]         120
├─Embedding: 1-2                         [32, 35, 200, 24]         1,152
├─Embedding: 1-3                         [32, 35, 200, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [32, 35, 200, 152]        --
│    │    └─AttentionLayer: 3-1          [32, 200, 35, 152]        93,024
│    │    └─Dropout: 3-2                 [32, 200, 35, 152]        --
│    │    └─LayerNorm: 3-3               [32, 200, 35, 152]        304
│    │    └─Sequential: 3-4              [32, 200, 35, 152]        78,232
│    │    └─Dropout: 3-5                 [32, 200, 35, 152]        --
│    │    └─LayerNorm: 3-6               [32, 200, 35, 152]        304
│    └─SelfAttentionLayer: 2-2           [32, 35, 200, 152]        --
│    │    └─AttentionLayer: 3-7          [32, 200, 35, 152]        93,024
│    │    └─Dropout: 3-8                 [32, 200, 35, 152]        --
│    │    └─LayerNorm: 3-9               [32, 200, 35, 152]        304
│    │    └─Sequential: 3-10             [32, 200, 35, 152]        78,232
│    │    └─Dropout: 3-11                [32, 200, 35, 152]        --
│    │    └─LayerNorm: 3-12              [32, 200, 35, 152]        304
│    └─SelfAttentionLayer: 2-3           [32, 35, 200, 152]        --
│    │    └─AttentionLayer: 3-13         [32, 200, 35, 152]        93,024
│    │    └─Dropout: 3-14                [32, 200, 35, 152]        --
│    │    └─LayerNorm: 3-15              [32, 200, 35, 152]        304
│    │    └─Sequential: 3-16             [32, 200, 35, 152]        78,232
│    │    └─Dropout: 3-17                [32, 200, 35, 152]        --
│    │    └─LayerNorm: 3-18              [32, 200, 35, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [32, 35, 200, 152]        --
│    │    └─AttentionLayer: 3-19         [32, 35, 200, 152]        93,024
│    │    └─Dropout: 3-20                [32, 35, 200, 152]        --
│    │    └─LayerNorm: 3-21              [32, 35, 200, 152]        304
│    │    └─Sequential: 3-22             [32, 35, 200, 152]        78,232
│    │    └─Dropout: 3-23                [32, 35, 200, 152]        --
│    │    └─LayerNorm: 3-24              [32, 35, 200, 152]        304
│    └─SelfAttentionLayer: 2-5           [32, 35, 200, 152]        --
│    │    └─AttentionLayer: 3-25         [32, 35, 200, 152]        93,024
│    │    └─Dropout: 3-26                [32, 35, 200, 152]        --
│    │    └─LayerNorm: 3-27              [32, 35, 200, 152]        304
│    │    └─Sequential: 3-28             [32, 35, 200, 152]        78,232
│    │    └─Dropout: 3-29                [32, 35, 200, 152]        --
│    │    └─LayerNorm: 3-30              [32, 35, 200, 152]        304
│    └─SelfAttentionLayer: 2-6           [32, 35, 200, 152]        --
│    │    └─AttentionLayer: 3-31         [32, 35, 200, 152]        93,024
│    │    └─Dropout: 3-32                [32, 35, 200, 152]        --
│    │    └─LayerNorm: 3-33              [32, 35, 200, 152]        304
│    │    └─Sequential: 3-34             [32, 35, 200, 152]        78,232
│    │    └─Dropout: 3-35                [32, 35, 200, 152]        --
│    │    └─LayerNorm: 3-36              [32, 35, 200, 152]        304
├─Linear: 1-6                            [32, 200, 2]              10,642
==========================================================================================
Total params: 1,603,266
Trainable params: 1,603,266
Non-trainable params: 0
Total mult-adds (M): 33.38
==========================================================================================
Input size (MB): 3.58
Forward/backward pass size (MB): 14321.77
Params size (MB): 4.17
Estimated Total Size (MB): 14329.52
==========================================================================================

Loss: MaskedMAELossNYC

2024-05-06 06:14:54.817163 Epoch 1  	Train Loss = 117.10686 Val Loss = 71.80730
2024-05-06 06:15:15.919587 Epoch 2  	Train Loss = 51.42404 Val Loss = 38.14805
2024-05-06 06:15:37.628513 Epoch 3  	Train Loss = 29.56737 Val Loss = 25.17479
2024-05-06 06:15:59.074377 Epoch 4  	Train Loss = 23.56938 Val Loss = 19.39808
2024-05-06 06:16:20.456051 Epoch 5  	Train Loss = 20.41575 Val Loss = 19.73567
2024-05-06 06:16:41.825877 Epoch 6  	Train Loss = 17.87999 Val Loss = 16.36281
2024-05-06 06:17:03.196926 Epoch 7  	Train Loss = 16.96128 Val Loss = 14.24847
2024-05-06 06:17:24.542200 Epoch 8  	Train Loss = 15.38902 Val Loss = 15.73123
2024-05-06 06:17:45.899791 Epoch 9  	Train Loss = 15.59243 Val Loss = 14.42911
2024-05-06 06:18:07.218710 Epoch 10  	Train Loss = 15.30211 Val Loss = 16.61940
2024-05-06 06:18:28.540154 Epoch 11  	Train Loss = 14.29262 Val Loss = 13.24709
2024-05-06 06:18:49.880204 Epoch 12  	Train Loss = 13.35936 Val Loss = 13.33664
2024-05-06 06:19:11.207124 Epoch 13  	Train Loss = 13.96277 Val Loss = 14.89624
2024-05-06 06:19:32.528820 Epoch 14  	Train Loss = 13.21814 Val Loss = 14.19336
2024-05-06 06:19:53.858603 Epoch 15  	Train Loss = 13.06019 Val Loss = 12.97650
2024-05-06 06:20:15.181529 Epoch 16  	Train Loss = 12.95352 Val Loss = 17.27946
2024-05-06 06:20:36.485102 Epoch 17  	Train Loss = 13.23687 Val Loss = 13.80449
2024-05-06 06:20:57.802834 Epoch 18  	Train Loss = 12.23485 Val Loss = 13.15799
2024-05-06 06:21:19.111828 Epoch 19  	Train Loss = 12.38122 Val Loss = 12.29181
2024-05-06 06:21:40.438174 Epoch 20  	Train Loss = 11.67524 Val Loss = 12.21980
2024-05-06 06:22:01.775302 Epoch 21  	Train Loss = 10.60270 Val Loss = 11.89904
2024-05-06 06:22:23.112194 Epoch 22  	Train Loss = 10.44771 Val Loss = 11.86017
2024-05-06 06:22:44.443757 Epoch 23  	Train Loss = 10.38889 Val Loss = 11.97760
2024-05-06 06:23:05.802818 Epoch 24  	Train Loss = 10.34775 Val Loss = 11.88693
2024-05-06 06:23:27.144402 Epoch 25  	Train Loss = 10.33436 Val Loss = 11.80554
2024-05-06 06:23:48.476849 Epoch 26  	Train Loss = 10.32406 Val Loss = 11.90636
2024-05-06 06:24:09.819071 Epoch 27  	Train Loss = 10.29363 Val Loss = 11.72820
2024-05-06 06:24:31.154143 Epoch 28  	Train Loss = 10.27718 Val Loss = 11.88527
2024-05-06 06:24:52.500288 Epoch 29  	Train Loss = 10.24190 Val Loss = 11.80549
2024-05-06 06:25:13.859530 Epoch 30  	Train Loss = 10.22488 Val Loss = 11.72330
2024-05-06 06:25:35.190448 Epoch 31  	Train Loss = 10.14157 Val Loss = 11.83886
2024-05-06 06:25:56.519922 Epoch 32  	Train Loss = 10.13322 Val Loss = 11.81913
2024-05-06 06:26:17.857156 Epoch 33  	Train Loss = 10.13622 Val Loss = 11.78200
2024-05-06 06:26:39.168085 Epoch 34  	Train Loss = 10.11193 Val Loss = 11.80636
2024-05-06 06:27:00.497802 Epoch 35  	Train Loss = 10.10832 Val Loss = 11.78005
2024-05-06 06:27:21.810712 Epoch 36  	Train Loss = 10.11787 Val Loss = 11.78962
2024-05-06 06:27:43.128020 Epoch 37  	Train Loss = 10.11737 Val Loss = 11.78894
2024-05-06 06:28:04.461427 Epoch 38  	Train Loss = 10.09837 Val Loss = 11.77208
2024-05-06 06:28:25.777553 Epoch 39  	Train Loss = 10.11336 Val Loss = 11.77118
2024-05-06 06:28:47.116573 Epoch 40  	Train Loss = 10.10288 Val Loss = 11.82446
2024-05-06 06:29:08.440315 Epoch 41  	Train Loss = 10.09346 Val Loss = 11.80113
2024-05-06 06:29:29.796532 Epoch 42  	Train Loss = 10.08849 Val Loss = 11.76323
2024-05-06 06:29:51.148804 Epoch 43  	Train Loss = 10.09818 Val Loss = 11.77930
2024-05-06 06:30:12.497704 Epoch 44  	Train Loss = 10.09365 Val Loss = 11.81528
2024-05-06 06:30:33.852504 Epoch 45  	Train Loss = 10.08636 Val Loss = 11.79643
2024-05-06 06:30:55.193981 Epoch 46  	Train Loss = 10.09266 Val Loss = 11.83007
2024-05-06 06:31:16.521238 Epoch 47  	Train Loss = 10.09589 Val Loss = 11.74761
2024-05-06 06:31:37.699588 Epoch 48  	Train Loss = 10.08238 Val Loss = 11.79618
2024-05-06 06:31:58.867763 Epoch 49  	Train Loss = 10.08877 Val Loss = 11.74822
2024-05-06 06:32:20.044441 Epoch 50  	Train Loss = 10.07342 Val Loss = 11.78119
2024-05-06 06:32:41.209242 Epoch 51  	Train Loss = 10.07703 Val Loss = 11.85594
2024-05-06 06:33:02.372320 Epoch 52  	Train Loss = 10.06754 Val Loss = 11.79517
2024-05-06 06:33:23.541461 Epoch 53  	Train Loss = 10.06411 Val Loss = 11.79761
2024-05-06 06:33:44.711428 Epoch 54  	Train Loss = 10.08132 Val Loss = 11.79073
2024-05-06 06:34:05.883915 Epoch 55  	Train Loss = 10.06851 Val Loss = 11.77775
2024-05-06 06:34:27.046098 Epoch 56  	Train Loss = 10.06234 Val Loss = 11.79711
2024-05-06 06:34:48.215114 Epoch 57  	Train Loss = 10.04874 Val Loss = 11.80955
2024-05-06 06:35:09.367515 Epoch 58  	Train Loss = 10.04136 Val Loss = 11.75963
2024-05-06 06:35:30.534049 Epoch 59  	Train Loss = 10.04011 Val Loss = 11.76928
2024-05-06 06:35:51.708306 Epoch 60  	Train Loss = 10.04646 Val Loss = 11.75511
Early stopping at epoch: 60
Best at epoch 30:
Train Loss = 10.22488
Train RMSE = 17.53763, MAE = 9.77032, MAPE = 15.86045
Val Loss = 11.72330
Val RMSE = 21.15463, MAE = 11.72636, MAPE = 18.28535
Saved Model: ../saved_models/STAEformer-NYCTaxi-2024-05-06-06-14-26.pt
--------- Test ---------
All Steps RMSE = 19.15101, MAE = 10.76649, MAPE = 16.82542
Step 1 RMSE = 19.15101, MAE = 10.76649, MAPE = 16.82542
Inference time: 2.05 s
